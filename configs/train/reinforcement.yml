reinforcement:
  enabled: true
  reward_model: "ollama:llama3"
  training_strategy: "ppo"
  batch_size: 4
  learning_rate: 2e-5
  epochs: 3
  sequence_length: 2048
  adapter_out: "./lora_rlhf"
