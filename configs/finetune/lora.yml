finetune:
  output_dir: "./outputs"
  batch_size: 4
  learning_rate: 2e-4
  epochs: 3
  sequence_length: 2048
  lora:
    r: 16
    lora_alpha: 32
    lora_dropout: 0.05
    adapter_path: "./lora_adapter"
